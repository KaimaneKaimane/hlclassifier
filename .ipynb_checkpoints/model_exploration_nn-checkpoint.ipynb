{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Kaimane\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Kaimane\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from sklearn.datasets import load_files\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "import pickle\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize  \n",
    "\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "stop_words = set(stopwords.words('english')) \n",
    "\n",
    "import gensim\n",
    "import gensim.downloader as api\n",
    "glove_embeddings = api.load('glove-twitter-25')\n",
    "#wv = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = Path.cwd() / Path('dataset/News_Category_Dataset_v2.json')\n",
    "df = pd.read_json(dataset_path, lines = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_df = df.drop(df[df['headline'].str.len() == 0].index, axis=0)\n",
    "\n",
    "reduced_df = reduced_df[['headline', 'category']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>there were 2 mass shootings in texas last week...</td>\n",
       "      <td>CRIME</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>will smith joins diplo and nicky jam for the 2...</td>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hugh grant marries for the first time at age 57</td>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>jim carrey blasts castrato adam schiff and dem...</td>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>julianna margulies uses donald trump poop bags...</td>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>morgan freeman devastated that sexual harassme...</td>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>donald trump is lovin new mcdonalds jingle in ...</td>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>what to watch on amazon prime thats new this week</td>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>mike myers reveals hed like to do a fourth aus...</td>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>what to watch on hulu thats new this week</td>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>justin timberlake visits texas school shooting...</td>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>south korean president meets north koreas kim ...</td>\n",
       "      <td>WORLD NEWS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>with its way of life at risk this remote oyste...</td>\n",
       "      <td>IMPACT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>trumps crackdown on immigrant parents puts mor...</td>\n",
       "      <td>POLITICS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>trumps son should be concerned fbi obtained wi...</td>\n",
       "      <td>POLITICS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>edward snowden theres no one trump loves more ...</td>\n",
       "      <td>POLITICS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>booyah obama photographer hilariously trolls t...</td>\n",
       "      <td>POLITICS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>ireland votes to repeal abortion amendment in ...</td>\n",
       "      <td>POLITICS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>ryan zinke looks to reel back some critics wit...</td>\n",
       "      <td>POLITICS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>trumps scottish golf resort pays women signifi...</td>\n",
       "      <td>POLITICS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>weird fathers day gifts your dad doesnt know h...</td>\n",
       "      <td>WEIRD NEWS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>twitter putstarwarsinotherfilms and it was uni...</td>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>mystery wolflike animal reportedly shot in mon...</td>\n",
       "      <td>WEIRD NEWS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>north korea still open to talks after trump ca...</td>\n",
       "      <td>WORLD NEWS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2 men detonate bomb inside indian restaurant n...</td>\n",
       "      <td>WORLD NEWS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>thousands travel home to ireland to vote on ab...</td>\n",
       "      <td>WORLD NEWS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>irish voters set to liberalize abortion laws i...</td>\n",
       "      <td>WORLD NEWS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>warriors coach steve kerr calls nfl ban on pro...</td>\n",
       "      <td>BLACK VOICES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>in historic victory barbados elects first fema...</td>\n",
       "      <td>BLACK VOICES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>police killed at least 378 black americans fro...</td>\n",
       "      <td>BLACK VOICES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200823</th>\n",
       "      <td>chris gregoire washington state governor discu...</td>\n",
       "      <td>QUEER VOICES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200824</th>\n",
       "      <td>glenn close on albert nobbs gender bending and...</td>\n",
       "      <td>QUEER VOICES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200825</th>\n",
       "      <td>tinker and change the world</td>\n",
       "      <td>IMPACT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200826</th>\n",
       "      <td>pregnant and displaced double the danger</td>\n",
       "      <td>IMPACT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200827</th>\n",
       "      <td>tom brady helps mentor tom martinez find a kid...</td>\n",
       "      <td>IMPACT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200828</th>\n",
       "      <td>boxer puppy and cows make friends during walk ...</td>\n",
       "      <td>ENVIRONMENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200829</th>\n",
       "      <td>black smoker vents new species discovered near...</td>\n",
       "      <td>ENVIRONMENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200830</th>\n",
       "      <td>green activists 50 and older</td>\n",
       "      <td>ENVIRONMENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200831</th>\n",
       "      <td>winter weather photo contest submit your own p...</td>\n",
       "      <td>ENVIRONMENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200832</th>\n",
       "      <td>insects top newly discovered species list</td>\n",
       "      <td>ENVIRONMENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200833</th>\n",
       "      <td>four more bank closures mark the week of janua...</td>\n",
       "      <td>BUSINESS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200834</th>\n",
       "      <td>everything you need to know about overdraft fe...</td>\n",
       "      <td>BUSINESS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200835</th>\n",
       "      <td>walmart waving goodbye to some greeters</td>\n",
       "      <td>BUSINESS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200836</th>\n",
       "      <td>at world economic forum fear of global contagi...</td>\n",
       "      <td>BUSINESS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200837</th>\n",
       "      <td>positive customer experience whats the return ...</td>\n",
       "      <td>BUSINESS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200838</th>\n",
       "      <td>sundance icet and shades of the american race ...</td>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200839</th>\n",
       "      <td>girl with the dragon tattoo india release canc...</td>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200840</th>\n",
       "      <td>dont think a look at the chemical brothers con...</td>\n",
       "      <td>CULTURE &amp; ARTS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200841</th>\n",
       "      <td>matthew marks discusses his new la gallery</td>\n",
       "      <td>CULTURE &amp; ARTS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200842</th>\n",
       "      <td>allard van hoorns urban songline explores rela...</td>\n",
       "      <td>CULTURE &amp; ARTS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200843</th>\n",
       "      <td>good games is it possible</td>\n",
       "      <td>TECH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200844</th>\n",
       "      <td>google now open for teens with some safeguards</td>\n",
       "      <td>TECH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200845</th>\n",
       "      <td>web wars</td>\n",
       "      <td>TECH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200846</th>\n",
       "      <td>first white house chief technology officer ane...</td>\n",
       "      <td>TECH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200847</th>\n",
       "      <td>watch the top 9 youtube videos of the week</td>\n",
       "      <td>TECH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200848</th>\n",
       "      <td>rim ceo thorsten heins significant plans for b...</td>\n",
       "      <td>TECH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200849</th>\n",
       "      <td>maria sharapova stunned by victoria azarenka i...</td>\n",
       "      <td>SPORTS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200850</th>\n",
       "      <td>giants over patriots jets over colts among mos...</td>\n",
       "      <td>SPORTS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200851</th>\n",
       "      <td>aldon smith arrested 49ers linebacker busted f...</td>\n",
       "      <td>SPORTS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200852</th>\n",
       "      <td>dwight howard rips teammates after magic loss ...</td>\n",
       "      <td>SPORTS</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200847 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 headline        category\n",
       "0       there were 2 mass shootings in texas last week...           CRIME\n",
       "1       will smith joins diplo and nicky jam for the 2...   ENTERTAINMENT\n",
       "2         hugh grant marries for the first time at age 57   ENTERTAINMENT\n",
       "3       jim carrey blasts castrato adam schiff and dem...   ENTERTAINMENT\n",
       "4       julianna margulies uses donald trump poop bags...   ENTERTAINMENT\n",
       "5       morgan freeman devastated that sexual harassme...   ENTERTAINMENT\n",
       "6       donald trump is lovin new mcdonalds jingle in ...   ENTERTAINMENT\n",
       "7       what to watch on amazon prime thats new this week   ENTERTAINMENT\n",
       "8       mike myers reveals hed like to do a fourth aus...   ENTERTAINMENT\n",
       "9               what to watch on hulu thats new this week   ENTERTAINMENT\n",
       "10      justin timberlake visits texas school shooting...   ENTERTAINMENT\n",
       "11      south korean president meets north koreas kim ...      WORLD NEWS\n",
       "12      with its way of life at risk this remote oyste...          IMPACT\n",
       "13      trumps crackdown on immigrant parents puts mor...        POLITICS\n",
       "14      trumps son should be concerned fbi obtained wi...        POLITICS\n",
       "15      edward snowden theres no one trump loves more ...        POLITICS\n",
       "16      booyah obama photographer hilariously trolls t...        POLITICS\n",
       "17      ireland votes to repeal abortion amendment in ...        POLITICS\n",
       "18      ryan zinke looks to reel back some critics wit...        POLITICS\n",
       "19      trumps scottish golf resort pays women signifi...        POLITICS\n",
       "20      weird fathers day gifts your dad doesnt know h...      WEIRD NEWS\n",
       "21      twitter putstarwarsinotherfilms and it was uni...   ENTERTAINMENT\n",
       "22      mystery wolflike animal reportedly shot in mon...      WEIRD NEWS\n",
       "23      north korea still open to talks after trump ca...      WORLD NEWS\n",
       "24      2 men detonate bomb inside indian restaurant n...      WORLD NEWS\n",
       "25      thousands travel home to ireland to vote on ab...      WORLD NEWS\n",
       "26      irish voters set to liberalize abortion laws i...      WORLD NEWS\n",
       "27      warriors coach steve kerr calls nfl ban on pro...    BLACK VOICES\n",
       "28      in historic victory barbados elects first fema...    BLACK VOICES\n",
       "29      police killed at least 378 black americans fro...    BLACK VOICES\n",
       "...                                                   ...             ...\n",
       "200823  chris gregoire washington state governor discu...    QUEER VOICES\n",
       "200824  glenn close on albert nobbs gender bending and...    QUEER VOICES\n",
       "200825                        tinker and change the world          IMPACT\n",
       "200826           pregnant and displaced double the danger          IMPACT\n",
       "200827  tom brady helps mentor tom martinez find a kid...          IMPACT\n",
       "200828  boxer puppy and cows make friends during walk ...     ENVIRONMENT\n",
       "200829  black smoker vents new species discovered near...     ENVIRONMENT\n",
       "200830                       green activists 50 and older     ENVIRONMENT\n",
       "200831  winter weather photo contest submit your own p...     ENVIRONMENT\n",
       "200832          insects top newly discovered species list     ENVIRONMENT\n",
       "200833  four more bank closures mark the week of janua...        BUSINESS\n",
       "200834  everything you need to know about overdraft fe...        BUSINESS\n",
       "200835            walmart waving goodbye to some greeters        BUSINESS\n",
       "200836  at world economic forum fear of global contagi...        BUSINESS\n",
       "200837  positive customer experience whats the return ...        BUSINESS\n",
       "200838  sundance icet and shades of the american race ...   ENTERTAINMENT\n",
       "200839  girl with the dragon tattoo india release canc...   ENTERTAINMENT\n",
       "200840  dont think a look at the chemical brothers con...  CULTURE & ARTS\n",
       "200841         matthew marks discusses his new la gallery  CULTURE & ARTS\n",
       "200842  allard van hoorns urban songline explores rela...  CULTURE & ARTS\n",
       "200843                          good games is it possible            TECH\n",
       "200844     google now open for teens with some safeguards            TECH\n",
       "200845                                           web wars            TECH\n",
       "200846  first white house chief technology officer ane...            TECH\n",
       "200847         watch the top 9 youtube videos of the week            TECH\n",
       "200848  rim ceo thorsten heins significant plans for b...            TECH\n",
       "200849  maria sharapova stunned by victoria azarenka i...          SPORTS\n",
       "200850  giants over patriots jets over colts among mos...          SPORTS\n",
       "200851  aldon smith arrested 49ers linebacker busted f...          SPORTS\n",
       "200852  dwight howard rips teammates after magic loss ...          SPORTS\n",
       "\n",
       "[200847 rows x 2 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_df = pd.DataFrame(reduced_df)\n",
    "\n",
    "def clean_headline(original_headline):\n",
    "    # remove special character\n",
    "    cleaned_headline = re.sub(r'[^A-Za-z0-9\\s]+', '',  original_headline)\n",
    "    cleaned_headline = re.sub(r'\\s+', ' ', cleaned_headline)\n",
    "    cleaned_headline = cleaned_headline.lower()\n",
    "    return cleaned_headline\n",
    "\n",
    "cleaned_df['headline'] = reduced_df['headline'].apply(clean_headline)\n",
    "\n",
    "cleaned_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply one hot encoding for categorical terms to avoid false similarities between unrelated categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  1  1 ... 10 10 10]\n",
      "200847\n",
      "[[1. 0. 0. ... 0. 0. 0.]\n",
      " [0. 1. 0. ... 0. 0. 0.]\n",
      " [0. 1. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# convert categories to integer\n",
    "unique_categories = cleaned_df['category'].unique()\n",
    "numbered_categories = {category_name: category_id for category_id, category_name in enumerate(unique_categories)}\n",
    "\n",
    "categories = cleaned_df['category'].apply(lambda category_name: numbered_categories[category_name])\n",
    "categories = np.array(categories)\n",
    "print(categories)\n",
    "\n",
    "# apply one hot encoding\n",
    "categories_encoded = to_categorical(categories)\n",
    "print(len(categories_encoded))\n",
    "print(categories_encoded)\n",
    "\n",
    "# invert encoding\n",
    "#inverted = np.argmax(categories_encoded[0])\n",
    "#print(inverted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check number of headline words to crop longer strings or pad shorter strings\n",
    "Necessary unless using a RNN\n",
    "(not done, check length after stopword removal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    200847.000000\n",
       "mean          9.509766\n",
       "std           3.072245\n",
       "min           0.000000\n",
       "25%           7.000000\n",
       "50%          10.000000\n",
       "75%          12.000000\n",
       "max          43.000000\n",
       "Name: totalwords, dtype: float64"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_df['totalwords'] = cleaned_df['headline'].str.split().str.len()\n",
    "cleaned_df['totalwords'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='totalwords'>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "cleaned_df[['headline', 'totalwords']].groupby(['totalwords']).count().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task mentioned to split the train test set with a 5% test set\n",
    "Stratified would probably better due to the distribution of the data (but task text wanted random)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 160677\n",
      "val: 30127\n",
      "test: 10043\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    np.array(cleaned_df['headline']), \n",
    "    np.array(categories_encoded), \n",
    "    test_size=0.05, # seperate 5 % test\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(\n",
    "    np.array(x_train), \n",
    "    np.array(y_train), \n",
    "    test_size=3/19, # this evens out to 80% train 15% validation\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print('train:', len(x_train))\n",
    "print('val:', len(x_val))\n",
    "print('test:', len(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to classify text I need to embbed the words as vectors\n",
    "\n",
    "I was looking at the word2vec embeddings by google because they have been trained on news sources, but they were quiete large so I decided to use glove first to try to build a working model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "def word_embeddings(word_list):\n",
    "    embeddings = []\n",
    "    for word in word_list:\n",
    "        try:\n",
    "            embedded_word = glove_embeddings[word]\n",
    "            embeddings.append(embedded_word)\n",
    "        except KeyError:\n",
    "            pass\n",
    "    return embeddings\n",
    "\n",
    "def preprocess_headline(cleaned_headline):\n",
    "    # tokenize and remove stopwords\n",
    "    word_tokens = word_tokenize(cleaned_headline)  \n",
    "    preprocessed_headline = [word for word in word_tokens if not word in stop_words]\n",
    "    preprocessed_headline = word_embeddings(preprocessed_headline)\n",
    "    return preprocessed_headline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([ 0.58806  ,  0.39977  ,  0.84307  , -0.063675 , -0.16753  ,\n",
      "        0.37486  ,  0.93533  , -1.0507   , -0.57041  , -0.027532 ,\n",
      "        0.76356  , -0.10157  , -3.6861   ,  1.3102   ,  0.0082721,\n",
      "       -0.51605  ,  0.34933  , -0.18487  , -0.19589  ,  0.31658  ,\n",
      "       -0.99501  ,  0.065084 ,  0.34442  , -0.99523  , -0.42175  ],\n",
      "      dtype=float32), array([-0.85877 , -0.25618 ,  0.47717 , -0.52192 ,  0.24368 , -0.44259 ,\n",
      "        1.2098  , -2.4435  , -0.33302 , -0.776   , -0.096845, -0.85163 ,\n",
      "       -1.7985  ,  0.78595 , -0.8803  ,  0.10122 , -0.066462, -1.3265  ,\n",
      "       -0.24864 ,  0.29457 , -0.25723 , -0.06883 , -1.729   , -0.80892 ,\n",
      "        0.17549 ], dtype=float32), array([-3.5359e-01,  3.6248e-01,  6.4438e-01, -1.6631e-01, -8.1867e-04,\n",
      "       -2.8424e-01,  6.8665e-01, -1.5670e-01,  1.0579e-02, -1.0834e+00,\n",
      "       -3.0052e-01,  1.2679e-01, -1.7526e+00, -4.8335e-01,  6.3915e-01,\n",
      "        5.5049e-01, -5.1045e-02,  7.0710e-02,  9.3275e-01,  7.1842e-01,\n",
      "       -4.1215e-01,  1.3524e-01,  4.3255e-02, -8.9489e-01,  6.9795e-01],\n",
      "      dtype=float32), array([-1.8172  , -0.10259 ,  0.91942 , -0.36189 ,  0.69904 , -0.64117 ,\n",
      "        1.0748  , -0.3317  ,  0.44001 , -0.79414 ,  0.37604 , -0.18515 ,\n",
      "       -3.411   ,  0.38558 , -0.62595 , -0.029376,  0.8272  , -1.1117  ,\n",
      "       -1.0965  ,  0.45703 , -0.78875 , -0.21261 , -1.3877  , -0.17221 ,\n",
      "       -0.47334 ], dtype=float32), array([ 0.48725 ,  0.12199 ,  1.0169  , -0.39238 ,  0.38714 , -0.058022,\n",
      "        1.2433  ,  0.47777 ,  0.22541 , -1.4957  , -0.57937 , -0.71278 ,\n",
      "       -4.3683  ,  0.16701 , -0.04286 , -0.12616 , -0.76605 , -0.42555 ,\n",
      "       -0.9231  , -0.48838 , -0.4577  ,  0.1036  , -0.066237, -0.2338  ,\n",
      "        0.80829 ], dtype=float32), array([ 0.20243 , -0.348   ,  0.21751 , -0.4144  ,  1.4284  , -0.099695,\n",
      "        0.35218 , -0.93391 ,  0.4247  , -0.94631 ,  0.75541 ,  1.0552  ,\n",
      "       -3.0827  , -0.1222  , -0.29429 ,  0.02223 ,  0.53957 ,  0.2018  ,\n",
      "        0.41528 ,  0.4022  , -0.84446 , -1.2065  , -0.35917 , -0.13528 ,\n",
      "       -0.84705 ], dtype=float32), array([-0.18469 ,  0.39978 , -0.042707, -0.05565 ,  0.44953 ,  0.46044 ,\n",
      "        1.3342  ,  0.27087 , -0.20495 , -0.52676 , -0.19003 , -0.13644 ,\n",
      "       -4.5969  , -0.37411 , -0.086688, -0.64449 ,  0.28982 , -0.9522  ,\n",
      "       -0.61824 ,  0.33503 , -0.45159 ,  0.49904 ,  0.54592 , -0.31448 ,\n",
      "       -0.12302 ], dtype=float32), array([ 0.27219 ,  0.14377 , -0.52445 , -0.21456 , -0.56524 , -0.90017 ,\n",
      "        1.2983  , -1.5268  ,  0.052652, -0.25862 ,  0.41089 , -0.80369 ,\n",
      "       -2.562   ,  0.038534, -0.24846 , -0.90102 ,  0.63319 , -0.13017 ,\n",
      "        0.2341  ,  1.0082  , -0.66721 ,  0.56813 , -1.3115  , -0.97451 ,\n",
      "        0.65846 ], dtype=float32), array([-1.8172  , -0.10259 ,  0.91942 , -0.36189 ,  0.69904 , -0.64117 ,\n",
      "        1.0748  , -0.3317  ,  0.44001 , -0.79414 ,  0.37604 , -0.18515 ,\n",
      "       -3.411   ,  0.38558 , -0.62595 , -0.029376,  0.8272  , -1.1117  ,\n",
      "       -1.0965  ,  0.45703 , -0.78875 , -0.21261 , -1.3877  , -0.17221 ,\n",
      "       -0.47334 ], dtype=float32), array([ 6.5635e-01,  4.1117e-02,  9.3452e-01, -1.4115e-01,  9.8254e-01,\n",
      "       -5.1396e-01,  1.1953e+00,  1.9252e+00,  1.9419e-01, -9.1384e-01,\n",
      "        5.9072e-01, -3.7080e-04, -3.5803e+00, -7.1096e-01, -2.0070e-03,\n",
      "        4.4468e-01,  1.3177e-03,  4.1788e-01, -8.9997e-01, -3.2894e-01,\n",
      "       -4.4435e-01, -8.1892e-01,  2.5267e-03, -9.4815e-01,  4.4134e-01],\n",
      "      dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "padding_length = 50\n",
    "\n",
    "x_train_embedded = list(map(preprocess_headline, x_train))\n",
    "x_val_embedded   = list(map(preprocess_headline, x_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_padded = pad_sequences(x_train_embedded, padding='post', maxlen=padding_length, dtype='float32')\n",
    "x_val_padded  = pad_sequences(x_val_embedded, padding='post', maxlen=padding_length, dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1193514"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = len(glove_embeddings.vocab)\n",
    "embedding_dim = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<gensim.models.keyedvectors.Word2VecKeyedVectors object at 0x0000023947EFC390>\n"
     ]
    }
   ],
   "source": [
    "print(glove_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-155-6ce775ac54b9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[0membedding_vector\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mglove_embeddings\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m         \u001b[0membedding_matrix\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0membedding_vector\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices"
     ]
    }
   ],
   "source": [
    "embedding_matrix = np.zeros((vocab_size + 1, embedding_dim))\n",
    "for word, i in glove_embeddings.vocab.items():\n",
    "    try:\n",
    "        embedding_vector = glove_embeddings[word]\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    except KeyError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Negative dimension size caused by subtracting 5 from 1 for '{{node conv1d_17/conv1d}} = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", dilations=[1, 1, 1, 1], explicit_paddings=[], padding=\"VALID\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true](conv1d_17/conv1d/ExpandDims, conv1d_17/conv1d/ExpandDims_1)' with input shapes: [?,1,1,128], [1,5,128,128].",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32mc:\\users\\kaimane\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_create_c_op\u001b[1;34m(graph, node_def, inputs, control_inputs, op_def)\u001b[0m\n\u001b[0;32m   1811\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1812\u001b[1;33m     \u001b[0mc_op\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpywrap_tf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_FinishOperation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop_desc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1813\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: Negative dimension size caused by subtracting 5 from 1 for '{{node conv1d_17/conv1d}} = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", dilations=[1, 1, 1, 1], explicit_paddings=[], padding=\"VALID\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true](conv1d_17/conv1d/ExpandDims, conv1d_17/conv1d/ExpandDims_1)' with input shapes: [?,1,1,128], [1,5,128,128].",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-142-8332de83ea44>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mConv1D\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'relu'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMaxPooling1D\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mConv1D\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'relu'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMaxPooling1D\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m35\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# global max pooling\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFlatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\kaimane\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    924\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0m_in_functional_construction_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    925\u001b[0m       return self._functional_construction_call(inputs, args, kwargs,\n\u001b[1;32m--> 926\u001b[1;33m                                                 input_list)\n\u001b[0m\u001b[0;32m    927\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    928\u001b[0m     \u001b[1;31m# Maintains info about the `Layer.call` stack.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\kaimane\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m_functional_construction_call\u001b[1;34m(self, inputs, args, kwargs, input_list)\u001b[0m\n\u001b[0;32m   1115\u001b[0m           \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1116\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menable_auto_cast_variables\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_compute_dtype_object\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1117\u001b[1;33m               \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1118\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1119\u001b[0m           \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOperatorNotAllowedInGraphError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\kaimane\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\convolutional.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m    245\u001b[0m       \u001b[0minputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_compute_causal_padding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    246\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 247\u001b[1;33m     \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_convolution_op\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkernel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    248\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    249\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muse_bias\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\kaimane\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    199\u001b[0m     \u001b[1;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    200\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 201\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    202\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m       \u001b[1;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\kaimane\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\u001b[0m in \u001b[0;36mconvolution_v2\u001b[1;34m(input, filters, strides, padding, data_format, dilations, name)\u001b[0m\n\u001b[0;32m   1015\u001b[0m       \u001b[0mdata_format\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata_format\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1016\u001b[0m       \u001b[0mdilations\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdilations\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1017\u001b[1;33m       name=name)\n\u001b[0m\u001b[0;32m   1018\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1019\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\kaimane\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\u001b[0m in \u001b[0;36mconvolution_internal\u001b[1;34m(input, filters, strides, padding, data_format, dilations, name, call_from_convolution, num_spatial_dims)\u001b[0m\n\u001b[0;32m   1145\u001b[0m           \u001b[0mdata_format\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata_format\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1146\u001b[0m           \u001b[0mdilations\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdilations\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1147\u001b[1;33m           name=name)\n\u001b[0m\u001b[0;32m   1148\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1149\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mchannel_index\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\kaimane\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    199\u001b[0m     \u001b[1;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    200\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 201\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    202\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m       \u001b[1;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\kaimane\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    572\u001b[0m                   \u001b[0mfunc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__module__\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg_value\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'in a future version'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    573\u001b[0m                   if date is None else ('after %s' % date), instructions)\n\u001b[1;32m--> 574\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    575\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    576\u001b[0m     doc = _add_deprecated_arg_value_notice_to_docstring(\n",
      "\u001b[1;32mc:\\users\\kaimane\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    572\u001b[0m                   \u001b[0mfunc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__module__\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg_value\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'in a future version'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    573\u001b[0m                   if date is None else ('after %s' % date), instructions)\n\u001b[1;32m--> 574\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    575\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    576\u001b[0m     doc = _add_deprecated_arg_value_notice_to_docstring(\n",
      "\u001b[1;32mc:\\users\\kaimane\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\u001b[0m in \u001b[0;36mconv1d\u001b[1;34m(value, filters, stride, padding, use_cudnn_on_gpu, data_format, name, input, dilations)\u001b[0m\n\u001b[0;32m   1886\u001b[0m           \u001b[0mdata_format\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata_format\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1887\u001b[0m           \u001b[0mdilations\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdilations\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1888\u001b[1;33m           name=name)\n\u001b[0m\u001b[0;32m   1889\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1890\u001b[0m       result = squeeze_batch_dims(\n",
      "\u001b[1;32mc:\\users\\kaimane\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\gen_nn_ops.py\u001b[0m in \u001b[0;36mconv2d\u001b[1;34m(input, filter, strides, padding, use_cudnn_on_gpu, explicit_paddings, data_format, dilations, name)\u001b[0m\n\u001b[0;32m    977\u001b[0m                   \u001b[0mpadding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpadding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    978\u001b[0m                   \u001b[0mexplicit_paddings\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mexplicit_paddings\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 979\u001b[1;33m                   data_format=data_format, dilations=dilations, name=name)\n\u001b[0m\u001b[0;32m    980\u001b[0m   \u001b[0m_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_outputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    981\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0m_execute\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmust_record_gradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\kaimane\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[1;34m(op_type_name, name, **keywords)\u001b[0m\n\u001b[0;32m    742\u001b[0m       op = g._create_op_internal(op_type_name, inputs, dtypes=None,\n\u001b[0;32m    743\u001b[0m                                  \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mscope\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_types\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 744\u001b[1;33m                                  attrs=attr_protos, op_def=op_def)\n\u001b[0m\u001b[0;32m    745\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    746\u001b[0m     \u001b[1;31m# `outputs` is returned as a separate return value so that the output\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\kaimane\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36m_create_op_internal\u001b[1;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_device)\u001b[0m\n\u001b[0;32m    591\u001b[0m     return super(FuncGraph, self)._create_op_internal(  # pylint: disable=protected-access\n\u001b[0;32m    592\u001b[0m         \u001b[0mop_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_types\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop_def\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 593\u001b[1;33m         compute_device)\n\u001b[0m\u001b[0;32m    594\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    595\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mcapture\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\kaimane\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_create_op_internal\u001b[1;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_device)\u001b[0m\n\u001b[0;32m   3483\u001b[0m           \u001b[0minput_types\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3484\u001b[0m           \u001b[0moriginal_op\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default_original_op\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3485\u001b[1;33m           op_def=op_def)\n\u001b[0m\u001b[0;32m   3486\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_op_helper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mret\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompute_device\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcompute_device\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3487\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\kaimane\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, node_def, g, inputs, output_types, control_inputs, input_types, original_op, op_def)\u001b[0m\n\u001b[0;32m   1973\u001b[0m         \u001b[0mop_def\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_op_def\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1974\u001b[0m       self._c_op = _create_c_op(self._graph, node_def, inputs,\n\u001b[1;32m-> 1975\u001b[1;33m                                 control_input_ops, op_def)\n\u001b[0m\u001b[0;32m   1976\u001b[0m       \u001b[0mname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_str\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1977\u001b[0m     \u001b[1;31m# pylint: enable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\kaimane\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_create_c_op\u001b[1;34m(graph, node_def, inputs, control_inputs, op_def)\u001b[0m\n\u001b[0;32m   1813\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1814\u001b[0m     \u001b[1;31m# Convert to ValueError for backwards compatibility.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1815\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1816\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1817\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mc_op\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Negative dimension size caused by subtracting 5 from 1 for '{{node conv1d_17/conv1d}} = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", dilations=[1, 1, 1, 1], explicit_paddings=[], padding=\"VALID\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true](conv1d_17/conv1d/ExpandDims, conv1d_17/conv1d/ExpandDims_1)' with input shapes: [?,1,1,128], [1,5,128,128]."
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, Conv1D, MaxPooling1D, Flatten, Dense, Embedding\n",
    "\n",
    "sequence_input = Input(shape=(padding_length,), dtype='float32')\n",
    "\n",
    "embedding_layer = Embedding(\n",
    "    vocab_size + 1,\n",
    "    embedding_dim,\n",
    "    weights=[embedding_matrix],\n",
    "    input_length=MAX_SEQUENCE_LENGTH,\n",
    "    trainable=False\n",
    ")\n",
    "\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "x = Conv1D(128, 5, activation='relu')(embedded_sequences)\n",
    "x = MaxPooling1D(5)(x)\n",
    "x = Conv1D(128, 5, activation='relu')(x)\n",
    "x = MaxPooling1D(5)(x)\n",
    "x = Conv1D(128, 5, activation='relu')(x)\n",
    "x = MaxPooling1D(35)(x)  # global max pooling\n",
    "x = Flatten()(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "preds = Dense(len(labels_index), activation='softmax')(x)\n",
    "\n",
    "model = Model(sequence_input, preds)\n",
    "model.summary()\n",
    "\n",
    "# model.compile(\n",
    "#     loss='categorical_crossentropy',\n",
    "#     optimizer='adam',\n",
    "#     metrics=['acc']\n",
    "# )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# happy learning!\n",
    "model.fit(x_train_padded, y_train, validation_data=(x_val_padded, y_val),\n",
    "          epochs=2, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Error converting shape to a TensorShape: Dimension value must be integer or None or have an __index__ method, got value '(50, 25)' with type '<class 'tuple'>'.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\users\\kaimane\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mmake_shape\u001b[1;34m(v, arg_name)\u001b[0m\n\u001b[0;32m    210\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 211\u001b[1;33m     \u001b[0mshape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtensor_shape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    212\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\kaimane\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\framework\\tensor_shape.py\u001b[0m in \u001b[0;36mas_shape\u001b[1;34m(shape)\u001b[0m\n\u001b[0;32m   1234\u001b[0m   \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1235\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mTensorShape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1236\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\kaimane\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\framework\\tensor_shape.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, dims)\u001b[0m\n\u001b[0;32m    755\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdims\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtuple\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# Most common case.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 756\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dims\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mDimension\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdims\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    757\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mdims\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\kaimane\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\framework\\tensor_shape.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    755\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdims\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtuple\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# Most common case.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 756\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dims\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mDimension\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdims\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    757\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mdims\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\kaimane\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\framework\\tensor_shape.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, value)\u001b[0m\n\u001b[0;32m    203\u001b[0m                       \u001b[1;34m\"an __index__ method, got value '{0!r}' with type '{1!r}'\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 204\u001b[1;33m                       .format(value, type(value))), None)\n\u001b[0m\u001b[0;32m    205\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_value\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\kaimane\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\six.py\u001b[0m in \u001b[0;36mraise_from\u001b[1;34m(value, from_value)\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: Dimension value must be integer or None or have an __index__ method, got value '(50, 25)' with type '<class 'tuple'>'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-132-7ad0e729d58f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[0minput_dim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0moutput_dim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0membedding_dim\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m         \u001b[0minput_length\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpadding_length\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m25\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m     )\n\u001b[0;32m     11\u001b[0m )\n",
      "\u001b[1;32mc:\\users\\kaimane\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\base.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    455\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    456\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 457\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    458\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    459\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprevious_value\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\kaimane\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py\u001b[0m in \u001b[0;36madd\u001b[1;34m(self, layer)\u001b[0m\n\u001b[0;32m    200\u001b[0m           \u001b[1;31m# Instantiate an input layer.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m           x = input_layer.Input(\n\u001b[1;32m--> 202\u001b[1;33m               batch_shape=batch_shape, dtype=dtype, name=layer.name + '_input')\n\u001b[0m\u001b[0;32m    203\u001b[0m           \u001b[1;31m# This will build the current layer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    204\u001b[0m           \u001b[1;31m# and create the node connecting the current layer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\kaimane\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\input_layer.py\u001b[0m in \u001b[0;36mInput\u001b[1;34m(shape, batch_size, name, dtype, sparse, tensor, ragged, **kwargs)\u001b[0m\n\u001b[0;32m    309\u001b[0m     input_layer_config.update(\n\u001b[0;32m    310\u001b[0m         {'batch_size': batch_size, 'input_shape': shape})\n\u001b[1;32m--> 311\u001b[1;33m   \u001b[0minput_layer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mInputLayer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0minput_layer_config\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    312\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m   \u001b[1;31m# Return tensor including `_keras_history`.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\kaimane\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\input_layer.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, input_shape, batch_size, dtype, input_tensor, sparse, name, ragged, **kwargs)\u001b[0m\n\u001b[0;32m    158\u001b[0m             \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    159\u001b[0m             \u001b[0msparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msparse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 160\u001b[1;33m             ragged=ragged)\n\u001b[0m\u001b[0;32m    161\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    162\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_placeholder\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\kaimane\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py\u001b[0m in \u001b[0;36mplaceholder\u001b[1;34m(shape, ndim, dtype, sparse, name, ragged)\u001b[0m\n\u001b[0;32m   1221\u001b[0m                                expand_composites=True)\n\u001b[0;32m   1222\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1223\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1224\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1225\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\kaimane\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py\u001b[0m in \u001b[0;36mplaceholder\u001b[1;34m(dtype, shape, name)\u001b[0m\n\u001b[0;32m   3098\u001b[0m                        \"eager execution.\")\n\u001b[0;32m   3099\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3100\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mgen_array_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3102\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\kaimane\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py\u001b[0m in \u001b[0;36mplaceholder\u001b[1;34m(dtype, shape, name)\u001b[0m\n\u001b[0;32m   6805\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mshape\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6806\u001b[0m     \u001b[0mshape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 6807\u001b[1;33m   \u001b[0mshape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_execute\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"shape\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   6808\u001b[0m   _, _, _op, _outputs = _op_def_library._apply_op_helper(\n\u001b[0;32m   6809\u001b[0m         \"Placeholder\", dtype=dtype, shape=shape, name=name)\n",
      "\u001b[1;32mc:\\users\\kaimane\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mmake_shape\u001b[1;34m(v, arg_name)\u001b[0m\n\u001b[0;32m    211\u001b[0m     \u001b[0mshape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtensor_shape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    212\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 213\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Error converting %s to a TensorShape: %s.\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0marg_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    214\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    215\u001b[0m     raise ValueError(\"Error converting %s to a TensorShape: %s.\" % (arg_name,\n",
      "\u001b[1;31mTypeError\u001b[0m: Error converting shape to a TensorShape: Dimension value must be integer or None or have an __index__ method, got value '(50, 25)' with type '<class 'tuple'>'."
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "model = Sequential()\n",
    "model.add(\n",
    "    layers.Embedding(\n",
    "        input_dim=vocab_size, \n",
    "        output_dim=embedding_dim, \n",
    "        input_length=padding_length\n",
    "    )\n",
    ")\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(10, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.58806 ,  0.39977 ,  0.84307 , ...,  0.34442 , -0.99523 ,\n",
       "        -0.42175 ],\n",
       "       [-0.85877 , -0.25618 ,  0.47717 , ..., -1.729   , -0.80892 ,\n",
       "         0.17549 ],\n",
       "       [-0.35359 ,  0.36248 ,  0.64438 , ...,  0.043255, -0.89489 ,\n",
       "         0.69795 ],\n",
       "       ...,\n",
       "       [ 0.      ,  0.      ,  0.      , ...,  0.      ,  0.      ,\n",
       "         0.      ],\n",
       "       [ 0.      ,  0.      ,  0.      , ...,  0.      ,  0.      ,\n",
       "         0.      ],\n",
       "       [ 0.      ,  0.      ,  0.      , ...,  0.      ,  0.      ,\n",
       "         0.      ]], dtype=float32)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_padded[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model was constructed with shape (None, 50) for input Tensor(\"embedding_6_input:0\", shape=(None, 50), dtype=float32), but it was called on an input with incompatible shape (None, 50, 25).\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    c:\\users\\kaimane\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:806 train_function  *\n        return step_function(self, iterator)\n    c:\\users\\kaimane\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:796 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    c:\\users\\kaimane\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:1211 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    c:\\users\\kaimane\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2585 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    c:\\users\\kaimane\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2945 _call_for_each_replica\n        return fn(*args, **kwargs)\n    c:\\users\\kaimane\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:789 run_step  **\n        outputs = model.train_step(data)\n    c:\\users\\kaimane\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:747 train_step\n        y_pred = self(x, training=True)\n    c:\\users\\kaimane\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py:985 __call__\n        outputs = call_fn(inputs, *args, **kwargs)\n    c:\\users\\kaimane\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:372 call\n        return super(Sequential, self).call(inputs, training=training, mask=mask)\n    c:\\users\\kaimane\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\functional.py:386 call\n        inputs, training=training, mask=mask)\n    c:\\users\\kaimane\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\functional.py:508 _run_internal_graph\n        outputs = node.layer(*args, **kwargs)\n    c:\\users\\kaimane\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py:976 __call__\n        self.name)\n    c:\\users\\kaimane\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\input_spec.py:216 assert_input_compatibility\n        ' but received input with shape ' + str(shape))\n\n    ValueError: Input 0 of layer dense_12 is incompatible with the layer: expected axis -1 of input shape to have value 2500 but received input with shape [None, 62500]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-129-9fe0999010c9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_val_padded\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     batch_size=10)\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;31m# loss, accuracy = model.evaluate(X_train, y_train, verbose=False)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;31m# print(\"Training Accuracy: {:.4f}\".format(accuracy))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\kaimane\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    106\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m     \u001b[1;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\kaimane\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[0;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1098\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1099\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\kaimane\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    778\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 780\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    781\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\kaimane\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    821\u001b[0m       \u001b[1;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    822\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 823\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    824\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    825\u001b[0m       \u001b[1;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\kaimane\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[1;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[0;32m    695\u001b[0m     self._concrete_stateful_fn = (\n\u001b[0;32m    696\u001b[0m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[1;32m--> 697\u001b[1;33m             *args, **kwds))\n\u001b[0m\u001b[0;32m    698\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    699\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0minvalid_creator_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0munused_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0munused_kwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\kaimane\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2853\u001b[0m       \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2854\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2855\u001b[1;33m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2856\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2857\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\kaimane\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   3211\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3212\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3213\u001b[1;33m       \u001b[0mgraph_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3214\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3215\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\kaimane\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[1;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m   3073\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3074\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3075\u001b[1;33m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[0;32m   3076\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3077\u001b[0m         \u001b[0mfunction_spec\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunction_spec\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\kaimane\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[1;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m    984\u001b[0m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    985\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 986\u001b[1;33m       \u001b[0mfunc_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    987\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    988\u001b[0m       \u001b[1;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\kaimane\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m    598\u001b[0m         \u001b[1;31m# __wrapped__ allows AutoGraph to swap in a converted function. We give\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    599\u001b[0m         \u001b[1;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 600\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    601\u001b[0m     \u001b[0mweak_wrapped_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mweakref\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mref\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwrapped_fn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    602\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\kaimane\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    971\u001b[0m           \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint:disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    972\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"ag_error_metadata\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 973\u001b[1;33m               \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    974\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    975\u001b[0m               \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    c:\\users\\kaimane\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:806 train_function  *\n        return step_function(self, iterator)\n    c:\\users\\kaimane\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:796 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    c:\\users\\kaimane\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:1211 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    c:\\users\\kaimane\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2585 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    c:\\users\\kaimane\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2945 _call_for_each_replica\n        return fn(*args, **kwargs)\n    c:\\users\\kaimane\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:789 run_step  **\n        outputs = model.train_step(data)\n    c:\\users\\kaimane\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:747 train_step\n        y_pred = self(x, training=True)\n    c:\\users\\kaimane\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py:985 __call__\n        outputs = call_fn(inputs, *args, **kwargs)\n    c:\\users\\kaimane\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:372 call\n        return super(Sequential, self).call(inputs, training=training, mask=mask)\n    c:\\users\\kaimane\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\functional.py:386 call\n        inputs, training=training, mask=mask)\n    c:\\users\\kaimane\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\functional.py:508 _run_internal_graph\n        outputs = node.layer(*args, **kwargs)\n    c:\\users\\kaimane\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py:976 __call__\n        self.name)\n    c:\\users\\kaimane\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\input_spec.py:216 assert_input_compatibility\n        ' but received input with shape ' + str(shape))\n\n    ValueError: Input 0 of layer dense_12 is incompatible with the layer: expected axis -1 of input shape to have value 2500 but received input with shape [None, 62500]\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    x_train_padded, \n",
    "    y_train,\n",
    "    epochs=10,\n",
    "    verbose=False,\n",
    "    validation_data=(x_val_padded, y_val),\n",
    "    batch_size=10)\n",
    "# loss, accuracy = model.evaluate(X_train, y_train, verbose=False)\n",
    "# print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
    "# loss, accuracy = model.evaluate(X_test, y_test, verbose=False)\n",
    "# print(\"Testing Accuracy:  {:.4f}\".format(accuracy))\n",
    "# plot_history(history)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
